{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_mass_classifier_PytorchLightning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e5NQwpSL_bW"
      },
      "source": [
        "# Dataset and Dependencies Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjAynKdO82Bl"
      },
      "source": [
        "# download dataset uploaded to my drive\n",
        "!pip3 install gdown\n",
        "!gdown https://drive.google.com/uc?id=1-jrCS61Ru-fRWUe46ot5ZvHNOsagm3eF\n",
        "!unzip /content/face_mask.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhbsRYJU89I7"
      },
      "source": [
        "# install dependencies\n",
        "!pip3 install pytorch-lightning timm easydict torch-metrics==1.1.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxLMEVyo9GVQ",
        "outputId": "2f6f00b7-a09f-4a7b-feef-fb79ea14175a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JIHMj6-MFZY"
      },
      "source": [
        "# Setting the Library and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCuFp1Ig9HYS"
      },
      "source": [
        "from easydict import EasyDict as edict\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP7Po8WHMNzJ"
      },
      "source": [
        "## Experiment Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvvFLfkC9JDS"
      },
      "source": [
        "dataset_cfg = {\"train\": {\n",
        "                  \"root\": \"/content/Face Mask Dataset/Train\",\n",
        "                  \"transform\": transforms.Compose([\n",
        "                                transforms.Resize((224, 224)),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.RandomRotation(10),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                ]),\n",
        "                   \"batch_size\": 16,\n",
        "                   \"shuffle\": True,\n",
        "                   \"num_workers\": 2\n",
        "                  },\n",
        "               \"val\": {\n",
        "                   \"root\": \"/content/Face Mask Dataset/Validation\",\n",
        "                   \"transform\": transforms.Compose([\n",
        "                                transforms.Resize((224, 224)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                ]),\n",
        "                   \"batch_size\": 1,\n",
        "                   \"shuffle\": False,\n",
        "                   \"num_workers\": 2\n",
        "               },\n",
        "               \"test\": {\n",
        "                   \"root\": \"/content/Face Mask Dataset/Test\",\n",
        "                   \"transform\": transforms.Compose([\n",
        "                                transforms.Resize((224, 224)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                ]),\n",
        "                   \"batch_size\": 1,\n",
        "                   \"shuffle\": False,\n",
        "                   \"num_workers\": 2\n",
        "               }}\n",
        "dataset_cfg = edict(dataset_cfg)\n",
        "\n",
        "# model configuration\n",
        "model_cfg = edict()\n",
        "model_cfg.backbone = \"mnasnet_100\" #backbone name {efficientnet_b0, mnasnet_100, resnet18, mobilenetv2_100}, refer to this https://rwightman.github.io/pytorch-image-models/models/\n",
        "model_cfg.num_embedding = 512\n",
        "model_cfg.pretrained = False # load pre-trained weight from Imagenet pre-trained\n",
        "model_cfg.num_classes = 1 # number of classes, 1 for mask/non-mask\n",
        "model_cfg.threshold = 0.5 # threshold for confidence outputs\n",
        "\n",
        "# trainer configuration\n",
        "trainer_cfg = edict()\n",
        "trainer_cfg.batch_size = 16\n",
        "trainer_cfg.seed = 0\n",
        "trainer_cfg.metrics = ['Accuracy', 'F1', 'Precision', 'Recall']\n",
        "trainer_cfg.lr = 0.001\n",
        "if model_cfg.pretrained == True:\n",
        "  trainer_cfg.experiment_name = f\"{model_cfg.backbone}_pretrained\"\n",
        "else:\n",
        "  trainer_cfg.experiment_name = f\"{model_cfg.backbone}_from_scratch\"\n",
        "\n",
        "trainer_cfg.weight_decay = 0.0005\n",
        "trainer_cfg.logs_path = \"/content/drive/MyDrive/tb_logs\"\n",
        "trainer_cfg.max_epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4QnA468M3nb"
      },
      "source": [
        "## Dataset and Dataloader Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ozl9qGw9SYn"
      },
      "source": [
        "# vars dataset\n",
        "datasets = edict()\n",
        "dataloaders = edict()\n",
        "\n",
        "# initialize datasets\n",
        "datasets.train = ImageFolder(root=dataset_cfg.train.root, \n",
        "                             transform=dataset_cfg.train.transform) \n",
        "datasets.val = ImageFolder(root=dataset_cfg.val.root, \n",
        "                           transform=dataset_cfg.val.transform)\n",
        "datasets.test = ImageFolder(root=dataset_cfg.test.root, \n",
        "                            transform=dataset_cfg.test.transform)\n",
        "\n",
        "# initialize dataloaders\n",
        "dataloaders.train = DataLoader(datasets.train, \n",
        "                               batch_size=dataset_cfg.train.batch_size, \n",
        "                               shuffle=dataset_cfg.train.shuffle, \n",
        "                               num_workers=dataset_cfg.train.num_workers)\n",
        "dataloaders.val = DataLoader(datasets.val, \n",
        "                             batch_size=dataset_cfg.val.batch_size, \n",
        "                             shuffle=dataset_cfg.val.shuffle, \n",
        "                             num_workers=dataset_cfg.val.num_workers)\n",
        "dataloaders.test = DataLoader(datasets.test, \n",
        "                              batch_size=dataset_cfg.test.batch_size, \n",
        "                              shuffle=dataset_cfg.test.shuffle, \n",
        "                              num_workers=dataset_cfg.test.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar4t8l3PM7jL"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr0qV3Os9S_P"
      },
      "source": [
        "from typing import Any, Callable, cast, Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "import timm\n",
        "import torchmetrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYvP0_E99VMk"
      },
      "source": [
        "class FaceMaskClassifier(pl.LightningModule):\n",
        "  def __init__(self, model_cfg: dict, trainer_cfg: dict):\n",
        "    super().__init__()\n",
        "\n",
        "    # set configs\n",
        "    self.save_hyperparameters()\n",
        "    self.backbone = timm.create_model(model_cfg.backbone,\n",
        "                                      pretrained=model_cfg.pretrained,\n",
        "                                      num_classes=model_cfg.num_classes)\n",
        "    self.hparams.model_cfg.threshold = torch.Tensor([self.hparams.model_cfg.threshold])\n",
        "    self.loss = nn.BCELoss()\n",
        "\n",
        "    # metrics\n",
        "    self.metrics = edict()\n",
        "    for mode in ['train', 'val', 'test']:\n",
        "      self.metrics[mode] = edict()\n",
        "      for metric in trainer_cfg.metrics:\n",
        "          self.metrics[mode][metric] = getattr(torchmetrics, metric)\n",
        "          if metric == \"Accuracy\":\n",
        "            self.metrics[mode][metric] = self.metrics[mode][metric](threshold=self.hparams.model_cfg.threshold)\n",
        "          else:\n",
        "            self.metrics[mode][metric] = getattr(torchmetrics, metric)(num_classes=model_cfg.num_classes, \n",
        "                                                                      threshold=self.hparams.model_cfg.threshold,\n",
        "                                                                      average=\"macro\")            \n",
        "    print(self.metrics)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.backbone(x)\n",
        "    return out\n",
        "  \n",
        "  def training_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    y = torch.unsqueeze(y, 1).to(torch.float32)\n",
        "\n",
        "    # inferencing\n",
        "    logits = self.forward(x)\n",
        "    y_hat = torch.sigmoid(logits)\n",
        "\n",
        "    # calculate loss value\n",
        "    loss = self.loss(y_hat, y)\n",
        "\n",
        "    # log loss\n",
        "    self.log('train_loss', loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "\n",
        "    # calculate metrics & log metrics\n",
        "    for metric in self.metrics.train:\n",
        "      y = y.to(torch.int)\n",
        "      self.log(f'train_{metric}', self.metrics.train[metric](y_hat.cpu(), y.cpu()), \n",
        "               prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    y = torch.unsqueeze(y, 1).to(torch.float32)\n",
        "\n",
        "    # inferencing\n",
        "    logits = self.forward(x)\n",
        "    y_hat = torch.sigmoid(logits)\n",
        "\n",
        "    # get prediction \n",
        "    results = edict()\n",
        "    results.logits = logits.cpu()\n",
        "    results.preds = y_hat.cpu()\n",
        "    results.y = y.cpu()\n",
        "\n",
        "    return results\n",
        "\n",
        "  def validation_epoch_end(self, results):\n",
        "\n",
        "    # get vars\n",
        "    len_results = len(results)\n",
        "    len_outputs = len(results[0])\n",
        "\n",
        "    # create prediction outputs tensor\n",
        "    outputs = edict()\n",
        "    for output in results[0]:\n",
        "      outputs[output] = torch.zeros((len_results, 1))\n",
        "    \n",
        "    for output in results[0]:\n",
        "      for i in range(len_results):\n",
        "        outputs[output][i, :] = results[i][output][0][0]\n",
        "\n",
        "    # measure val loss\n",
        "    loss_val = self.loss(outputs.preds, outputs.y)\n",
        "\n",
        "    # calculate metrics & log metrics\n",
        "    for metric in self.metrics.val:\n",
        "      y = outputs.y.to(torch.int)\n",
        "      self.logger.log_metrics({f'val_{metric}_epoch':self.metrics.val[metric](outputs.preds.cpu(), y.cpu())}, \n",
        "               step=self.current_epoch) \n",
        "    \n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "\n",
        "    # get vars\n",
        "    x, y = batch\n",
        "    y = torch.unsqueeze(y, 1).to(torch.float32)\n",
        "\n",
        "    # inferencing\n",
        "    logits = self.forward(x)\n",
        "    y_hat = torch.sigmoid(logits)\n",
        "\n",
        "    # get prediction \n",
        "    results = edict()\n",
        "    results.logits = logits.cpu()\n",
        "    results.preds = y_hat.cpu()\n",
        "    results.y = y.cpu()\n",
        "\n",
        "    return results\n",
        "\n",
        "  def test_epoch_end(self, results):\n",
        "  \n",
        "    # get vars\n",
        "    len_results = len(results)\n",
        "    len_outputs = len(results[0])\n",
        "\n",
        "    # create prediction outputs tensor\n",
        "    outputs = edict()\n",
        "    for output in results[0]:\n",
        "      outputs[output] = torch.zeros((len_results, 1))\n",
        "    \n",
        "    for output in results[0]:\n",
        "      for i in range(len_results):\n",
        "        outputs[output][i, :] = results[i][output][0][0]\n",
        "\n",
        "    loss_val = self.loss(outputs.preds, outputs.y)\n",
        "\n",
        "    # measure val loss\n",
        "    metrics = edict()\n",
        "    metrics.loss = float(loss_val.cpu().numpy())\n",
        "\n",
        "    # calculate metrics & log metrics\n",
        "    for metric in self.metrics.test:\n",
        "      y = outputs.y.to(torch.int)\n",
        "      metric_val = self.metrics.test[metric](outputs.preds.cpu(), y.cpu())\n",
        "      self.logger.log_metrics({f'test_{metric}_epoch':metric_val}, \n",
        "               step=self.current_epoch) \n",
        "      metrics[metric] = float(metric_val.cpu().numpy())\n",
        "    \n",
        "    print(metrics)\n",
        "    \n",
        "    return metrics\n",
        "    \n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.parameters(), lr=self.hparams.trainer_cfg.lr, \n",
        "                            weight_decay=self.hparams.trainer_cfg.weight_decay)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKOXxnKROBp_"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxmJ400K9XaN"
      },
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "# training\n",
        "# seed everything\n",
        "pl.seed_everything(trainer_cfg.seed)\n",
        "\n",
        "# logger\n",
        "logger = TensorBoardLogger(trainer_cfg.logs_path, name=trainer_cfg.experiment_name)\n",
        "\n",
        "# model\n",
        "model = FaceMaskClassifier(model_cfg=model_cfg, trainer_cfg=trainer_cfg)\n",
        "\n",
        "# trainer\n",
        "trainer = pl.Trainer(gpus=1, \n",
        "                     max_epochs=trainer_cfg.max_epochs,\n",
        "                     logger=logger)\n",
        "trainer.fit(model, dataloaders.train, dataloaders.val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh2dLrzsODOs"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gGNOj8S9Y6q"
      },
      "source": [
        "# # testing\n",
        "result = trainer.test(test_dataloaders=dataloaders.test)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt5_rpJ7OEbG"
      },
      "source": [
        "## Logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx11MXgj9aMh"
      },
      "source": [
        "# !kill 3952\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/tb_logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}